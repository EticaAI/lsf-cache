#item+conceptum+numerordinatio,#item+conceptum+codicem,#item+rem+i_eng+is_latn,#item+rem+i_qcc+is_zxxx+ix_codexfacto
1603:1:99:0:1603:1:7:2616:50,0_1603_1_7_2616_50,,EticaAI
1603:1:99:0:1603:1:7:2616:123,0_1603_1_7_2616_123,,EticaAI
1603:1:99:0:1603:1:7:2616:2479,0_1603_1_7_2616_2479,,CC0-1.0
1603:1:99:0:1603:1:7:2616:577,0_1603_1_7_2616_577,,2022-04-14
1603:1:99:0:1603:1:7:2616:7535,0_1603_1_7_2616_7535,,TODO: explain 1603_1_99
1603:1:99:1,1,fƒ´at l≈´x!,
1603:1:99:2,2,{% _üó£Ô∏è 1603_1_99_1 üó£Ô∏è_ %},
1603:1:99:2,2,The result of this section is a preview. We're aware it is not well (Abecedarium formatted for a book format. Sorry for the temporary inconvenience.,
1603:1:99:10,10,/General text hardcoded on the 1603_1.py/,
1603:1:99:10:1,10_1,"_**C≈çdex [{0}]**_ is the book format of the machine-readable dictionaries _**[{0}] {1}**_, which are distributed for implementers on external applications. This book is intended as advanced resource for other lexicographers and terminology translators, including detect and report inconsistencies.\n\nPractical lexicography is the art or craft of compiling, writing and editing dictionaries. The basics are not far different than a millennia ago: it is still a very humane, creative work. It is necessary to be humble: most of the translator's mistakes are, in fact, not translator's fault, but methodological flaws. Making sure of a source idea of what a concept represents, even if it means rewrite and make simpler, annex pictures, show examples, do whatever to make it be understood, makes even non-professional translators that care about their own language deliver better results than any alternative. In other words: even the so-called industry best practices of paying professional translators and reviewers cannot overcome already poorly explained source terms.\n\nThe initiative behind this compilation is also doing other dictionaries and accepts new suggestions of relevant topics on data exchange for humanitarian use. All have in common the fact that both have human translations and (if any) external interlingual codes related to each concept while making the end result explicitly already ready to be usable on average softwares. Naturally, each book version gives extensive explanations for collaborators on how to correct itself which become part of the next weekly release.",
1603:1:99:10:2,10_2,"Every book comes with several files both for book format (with (Abecedarium additional information) and machine-readable formats with Latinum) documentation of how to process them. If you receive this file and cannot find the alternatives, ask the human who provide this file.",
1603:1:99:10:3,10_3,"WARNING: Unless you are working with a natural language you understand it\'s letters and symbols, it is strongly advised to use automation to generate derived works. Keep manual human steps at minimum: if something goes wrong at least one or more languages can be used to verify mistakes. It's not at all necessary _know all languages_, but working with writing systems you don't understand is risky: copy and paste strategy can cause _additional_ human errors and is unlikely to get human review as fast as you would need.",
1603:1:99:10:4,10_4,TIP: The Asciidoctor (.adoc) is better at copy and pasting! It can be converted to other text formats.,
1603:1:99:10:5,10_5,"NOTE: /At the moment, there is no workflow to use https://www.wikidata.org/wiki/Wikidata:Lexicographical_data[Wikidata lexicographical data], which actually could be used as storage for stricter nomenclature. The current implementations use only Wikidata concepts, the Q-items./@eng-Latn",
1603:1:99:10:6,10_6,"The ***[{1}] {2}*** uses Wikidata as one strategy to conciliate language terms for one or more of it's concepts.\n\nThis means that this book, and related dictionaries data files require periodic updates to, at bare minimum, synchronize and re-share up to date translations.",
1603:1:99:10:7,10_7,"**How reliable are the community translations (Wikidata source)?**\n\nThe short, default answer is: **they are reliable**, even in cases of no authoritative translations for each subject.\n\nAs reference, it is likely a professional translator (without access to Wikipedia or Internal terminology bases of the control organizations) would deliver lower quality results if you do blind tests. This is possible because not just the average public, but even terminologists and professional translators help Wikipedia (and implicitly Wikidata).\n\nHowever, even when the result is correct, the current version needs improved differentiation, at minimum, acronym and long form. For major organizations, features such as __P1813 short names__ exist, but are not yet compiled with the current dataset.",
1603:1:99:10:8,10_8,"**Major reasons for ""wrong translations"" are not translators fault**\n\nTIP: As a rule of thumb, for already very defined concepts where you, as human, can manually verify one or more translated terms as a decent result, the other translations are likely to be acceptable. Dictionaries with edge cases (such as disputed territory names) would have further explanation.\n\nNOTE: Both at concept level and (as general statistics) book level, is planned to have indication concept likelihood of being well understood for very stricter translations initiatives.\n\nThe main reason for ""wrong translations"" are poorly defined concepts used to explain for community translators how to generate terminology translations. This would make existing translations from Wikidata (used not just by us) inconsistent. The second reason is if the dictionaries use translations for concepts without a strict match; in other words, if we make stricter definitions of what concept means but reuse Wikidada less exact terms. There are also issues when entire languages are encoded with wrong codes. Note that all these cases **wrong translations are strictly NOT translators fault, but lexicography fault**.\n\nIt is still possible to have strict translation level errors. But even if we point users how to correct Wikidata/Wikipedia (based on better contextual explanation of a concept, such as this book), the requirements to say the previous term was objectively a wrong human translation error (if following our seriousness on dictionary-building) are very high.",
1603:1:99:10:9,10_9,"From the point of view of data conciliation, the following methodology is used to release the terminology translations with the main concept table.\n\n. The main handcrafted lexicographical table (explained on previous topic), also provided on `{0}.no1.tm.hxl.csv`, may reference Wiki QID.\n. Every unique QID of  `{0}.no1.tm.hxl.csv`, together with language codes from [`1603:1:51`] (which requires knowing human languages), is used to prepare an SPARQL query optimized to run on https://query.wikidata.org/[Wikidata Query Service]. The query is so huge that it is not viable to ""Try it"" links (URL overlong), such https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service/queries/examples[as what you would find on Wikidata Tutorials], ***but*** it works!\n.. Note that the knowledge is free, the translations are there, but the multilingual humanitarian needs may lack people to prepare the files and shares then for general use.\n. The query result, with all QIDs and term labels, is shared as `{0}.wikiq.tm.hxl.csv`\n. The community reviewed translations of each singular QID is pre-compiled on an individual file `{0}.wikiq.tm.hxl.csv`\n. `{0}.no1.tm.hxl.csv` plus `{0}.wikiq.tm.hxl.csv` created `{0}.no11.tm.hxl.csv`",
1603:1:99:10:10,10_10,"This section explains the methodology of this book and it's machine readable formats. For your convenience the information used to explain the concepts (such as natural language and interlingual codes) which appears in this book are also summarized here. This approach is done both for reviews not needing to open other books (or deal with machine readable files) and also to spot errors on other dictionaries. +++<br><br>+++ About how the book and the dictionaries are compiled, a division of ""baseline concept table"" and (when relevant for a codex) ""translations conciliation"" is given different methodologies. +++<br><br>+++ Every book contains at minimum the baseline concept table and explanation of the used fields. This approach helps to release dictionaries faster while ensuring both humans and machines can know what to expect even when they are not ready to receive translations.",
1603:1:99:50,50,Quotes and other messages,
1603:1:99:50:1,50_1,/**Public domain means that each major common issue only needs to be resolved once**/@eng-Latn,
1603:1:99:999,999,@TODOs,
1603:1:99:999:1,999_1,Add links to codex to search by last edits on Q itens on the current book. See https://wikidata-todo.toolforge.org/sparql_rc.php?,
